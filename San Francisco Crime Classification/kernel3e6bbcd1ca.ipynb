{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/sf-crime/sampleSubmission.csv\n",
      "/kaggle/input/sf-crime/test.csv.zip\n",
      "/kaggle/input/sf-crime/train.csv.zip\n",
      "/kaggle/input/sf-crime/train.csv\n",
      "/kaggle/input/sf-crime/test.csv\n",
      "/kaggle/input/sf-crime/sf_map_copyright_openstreetmap_contributors.txt\n",
      "/kaggle/input/sf-crime/sf_map_copyright_openstreetmap_contributors.rds\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-efde1586ba7a>, line 102)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-efde1586ba7a>\"\u001b[0;36m, line \u001b[0;32m102\u001b[0m\n\u001b[0;31m    model = LGBMClassifier(num_leaves: 233,n_estimators=150,learning_rate=0.1)\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/sf-crime/train.csv', parse_dates=['Dates'])\n",
    "test = pd.read_csv('/kaggle/input/sf-crime/test.csv', parse_dates=['Dates'], index_col='Id')\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "max_words = 8000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(list(train[\"Address\"]) + list(test[\"Address\"]))\n",
    "\n",
    "haha = tokenizer.texts_to_sequences(train[\"Address\"])\n",
    "haha2 = tokenizer.texts_to_sequences(test[\"Address\"])\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "padded = pad_sequences(haha, maxlen=6)\n",
    "padded2 = pad_sequences(haha2, maxlen=6)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Flatten, Embedding, Dense \n",
    "model = Sequential()\n",
    "model.add(Embedding(padded.shape[0], 1, input_length=6))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(39, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le2 = LabelEncoder()\n",
    "y= le2.fit_transform(train['Category'])\n",
    "model.fit(padded, y, epochs = 10 , batch_size = 2048)\n",
    "\n",
    "preds = model.predict(padded)\n",
    "preds2 = model.predict(padded2)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca1 = PCA(n_components=2)\n",
    "X_low = pca1.fit_transform(preds)\n",
    "#X2 = pca1.inverse_transform(X_low)\n",
    "X_low2 = pca1.transform(preds2)\n",
    "\n",
    "train['Date'] = pd.to_datetime(train['Dates'].dt.date)\n",
    "train['n_days'] = (train['Date'] - train['Date'].min()).apply(lambda x: x.days)\n",
    "train['Day'] = train['Dates'].dt.day\n",
    "train['DayOfWeek'] = train['Dates'].dt.weekday\n",
    "train['Month'] = train['Dates'].dt.month\n",
    "train['Year'] = train['Dates'].dt.year\n",
    "train['Hour'] = train['Dates'].dt.hour\n",
    "train['Minute'] = train['Dates'].dt.minute\n",
    "train['Block'] = train['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "train['ST'] = train['Address'].str.contains('ST', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "test['Date'] = pd.to_datetime(test['Dates'].dt.date)\n",
    "test['n_days'] = (test['Date'] - test['Date'].min()).apply(lambda x: x.days)\n",
    "test['Day'] = test['Dates'].dt.day\n",
    "test['DayOfWeek'] = test['Dates'].dt.weekday\n",
    "test['Month'] = test['Dates'].dt.month\n",
    "test['Year'] = test['Dates'].dt.year\n",
    "test['Hour'] = test['Dates'].dt.hour\n",
    "test['Minute'] = test['Dates'].dt.minute\n",
    "test['Block'] = test['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "test['ST'] = test['Address'].str.contains('ST', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1 = LabelEncoder()\n",
    "train['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\n",
    "test['PdDistrict'] = le1.transform(test['PdDistrict'])\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "y= le2.fit_transform(train['Category'])\n",
    "\n",
    "train = pd.concat([train, pd.DataFrame(X_low)], 1)\n",
    "test = pd.concat([test, pd.DataFrame(X_low2)], 1)\n",
    "\n",
    "le3 = LabelEncoder()\n",
    "le3.fit(list(train['Address']) + list(test['Address']))\n",
    "train['Address'] = le3.transform(train['Address'])\n",
    "test['Address'] = le3.transform(test['Address'])\n",
    "\n",
    "train.drop(['Dates','Date','Descript','Resolution', 'Category'], 1, inplace=True)\n",
    "test.drop(['Dates','Date',], 1, inplace=True)\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "#hyper = {'colsample_bytree': 0.6254687581749554,\n",
    "# 'is_unbalance': False,\n",
    "# 'learning_rate': 0.10240007357745745,\n",
    "# 'min_child_samples': 105,\n",
    "# 'num_class': 39,\n",
    "# 'num_leaves': 233,\n",
    "# 'objective': 'multiclass',\n",
    "# 'reg_alpha': 0.4000134592012641,\n",
    "# 'reg_lambda': 0.5082596745249518,\n",
    "# 'subsample': 0.9338693244190213,\n",
    "# 'subsample_for_bin': 140000,\n",
    "# 'n_estimators': 150}\n",
    "model = LGBMClassifier(num_leaves: 233,n_estimators=150,learning_rate=0.1)\n",
    "model.fit(train, y, categorical_feature=[\"PdDistrict\", \"DayOfWeek\"])\n",
    "preds = model.predict_proba(test)\n",
    "submission = pd.DataFrame(preds, columns=le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')), index=test.index)\n",
    "submission.to_csv('LGBM_final.csv', index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
