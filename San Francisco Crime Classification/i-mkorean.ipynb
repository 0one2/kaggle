{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sf_map_copyright_openstreetmap_contributors.txt', 'test.csv', 'test.csv.zip', 'train.csv', 'sf_map_copyright_openstreetmap_contributors.rds', 'sampleSubmission.csv', 'train.csv.zip']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/10\n",
      "878049/878049 [==============================] - 6s 7us/step - loss: 3.1190 - acc: 0.1722\n",
      "Epoch 2/10\n",
      "878049/878049 [==============================] - 6s 7us/step - loss: 2.6841 - acc: 0.2009\n",
      "Epoch 3/10\n",
      "878049/878049 [==============================] - 6s 7us/step - loss: 2.6467 - acc: 0.2035\n",
      "Epoch 4/10\n",
      "878049/878049 [==============================] - 5s 6us/step - loss: 2.6171 - acc: 0.2124\n",
      "Epoch 5/10\n",
      "878049/878049 [==============================] - 6s 6us/step - loss: 2.6013 - acc: 0.2181\n",
      "Epoch 6/10\n",
      "878049/878049 [==============================] - 6s 6us/step - loss: 2.5945 - acc: 0.2203\n",
      "Epoch 7/10\n",
      "878049/878049 [==============================] - 6s 6us/step - loss: 2.5902 - acc: 0.2239\n",
      "Epoch 8/10\n",
      "878049/878049 [==============================] - 6s 6us/step - loss: 2.5866 - acc: 0.2274\n",
      "Epoch 9/10\n",
      "878049/878049 [==============================] - 6s 6us/step - loss: 2.5830 - acc: 0.2290\n",
      "Epoch 10/10\n",
      "878049/878049 [==============================] - 6s 6us/step - loss: 2.5794 - acc: 0.2306\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "train = pd.read_csv('../input/train.csv', parse_dates=['Dates'])\n",
    "test = pd.read_csv('../input/test.csv', parse_dates=['Dates'], index_col='Id')\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "max_words = 8000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(list(train[\"Address\"]) + list(test[\"Address\"]))\n",
    "\n",
    "haha = tokenizer.texts_to_sequences(train[\"Address\"])\n",
    "haha2 = tokenizer.texts_to_sequences(test[\"Address\"])\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "padded = pad_sequences(haha, maxlen=6)\n",
    "padded2 = pad_sequences(haha2, maxlen=6)\n",
    "\n",
    "from keras import Sequential\n",
    "from keras.layers import Flatten, Embedding, Dense \n",
    "model = Sequential()\n",
    "model.add(Embedding(padded.shape[0], 1, input_length=6))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(39, activation='softmax'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le2 = LabelEncoder()\n",
    "y= le2.fit_transform(train['Category'])\n",
    "model.fit(padded, y, epochs = 10 , batch_size = 2048)\n",
    "\n",
    "preds = model.predict(padded)\n",
    "preds2 = model.predict(padded2)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca1 = PCA(n_components=2)\n",
    "X_low = pca1.fit_transform(preds)\n",
    "#X2 = pca1.inverse_transform(X_low)\n",
    "X_low2 = pca1.transform(preds2)\n",
    "\n",
    "train['Date'] = pd.to_datetime(train['Dates'].dt.date)\n",
    "train['n_days'] = (train['Date'] - train['Date'].min()).apply(lambda x: x.days)\n",
    "train['Day'] = train['Dates'].dt.day\n",
    "train['DayOfWeek'] = train['Dates'].dt.weekday\n",
    "train['Month'] = train['Dates'].dt.month\n",
    "train['Year'] = train['Dates'].dt.year\n",
    "train['Hour'] = train['Dates'].dt.hour\n",
    "train['Minute'] = train['Dates'].dt.minute\n",
    "train['Block'] = train['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "train['ST'] = train['Address'].str.contains('ST', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "train[\"X_Y\"] = train[\"X\"] - train[\"Y\"]\n",
    "train[\"XY\"] = train[\"X\"] + train[\"Y\"]\n",
    "\n",
    "test['Date'] = pd.to_datetime(test['Dates'].dt.date)\n",
    "test['n_days'] = (test['Date'] - test['Date'].min()).apply(lambda x: x.days)\n",
    "test['Day'] = test['Dates'].dt.day\n",
    "test['DayOfWeek'] = test['Dates'].dt.weekday\n",
    "test['Month'] = test['Dates'].dt.month\n",
    "test['Year'] = test['Dates'].dt.year\n",
    "test['Hour'] = test['Dates'].dt.hour\n",
    "test['Minute'] = test['Dates'].dt.minute\n",
    "test['Block'] = test['Address'].str.contains('block', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "test['ST'] = test['Address'].str.contains('ST', case=False).apply(lambda x: 1 if x == True else 0)\n",
    "test[\"X_Y\"] = test[\"X\"] - test[\"Y\"]\n",
    "test[\"XY\"] = test[\"X\"] + test[\"Y\"]\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le1 = LabelEncoder()\n",
    "train['PdDistrict'] = le1.fit_transform(train['PdDistrict'])\n",
    "test['PdDistrict'] = le1.transform(test['PdDistrict'])\n",
    "\n",
    "le2 = LabelEncoder()\n",
    "y= le2.fit_transform(train['Category'])\n",
    "\n",
    "train = pd.concat([train, pd.DataFrame(X_low)], 1)\n",
    "test = pd.concat([test, pd.DataFrame(X_low2)], 1)\n",
    "\n",
    "le3 = LabelEncoder()\n",
    "le3.fit(list(train['Address']) + list(test['Address']))\n",
    "train['Address'] = le3.transform(train['Address'])\n",
    "test['Address'] = le3.transform(test['Address'])\n",
    "\n",
    "train.drop(['Dates','Date','Descript','Resolution', 'Category'], 1, inplace=True)\n",
    "test.drop(['Dates','Date',], 1, inplace=True)\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "hyper = {'colsample_bytree': 0.6254687581749554,\n",
    " 'is_unbalance': False,\n",
    " 'learning_rate': 0.10240007357745745,\n",
    " 'min_child_samples': 105,\n",
    " 'num_class': 39,\n",
    " 'num_leaves': 233,\n",
    " 'objective': 'multiclass',\n",
    " 'reg_alpha': 0.4000134592012641,\n",
    " 'reg_lambda': 0.5082596745249518,\n",
    " 'subsample': 0.9338693244190213,\n",
    " 'subsample_for_bin': 140000,\n",
    " 'n_estimators': 150}\n",
    "model = LGBMClassifier(**hyper\n",
    "                      )\n",
    "model.fit(train, y, categorical_feature=[\"PdDistrict\", \"DayOfWeek\"])\n",
    "preds = model.predict_proba(test)\n",
    "submission = pd.DataFrame(preds, columns=le2.inverse_transform(np.linspace(0, 38, 39, dtype='int16')), index=test.index)\n",
    "submission.to_csv('LGBM_final.csv', index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
